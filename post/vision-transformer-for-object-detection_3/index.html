<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Vision Transformer 在目标检测上的探索，DETR 系列文章解读（三）Effient DETR, PnP DETR, Sparse DETR - kinredon's blog</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="kinredon"><meta name=description content="前言 这篇文章主要分享几篇 DETR 和 Deformable DETR 的后续工作，包括 Effient DETR [1], PnP DETR [2], Sparse DETR [3]。 分享这几篇工作的主要原因在于它们都是 DETR 的一个方向的改进，即减少 DETR 中的"><meta name=keywords content="blog,computer vision,deep learning"><meta name=generator content="Hugo 0.95.0"><link rel=canonical href=https://kinredon.github.io/post/vision-transformer-for-object-detection_3/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.fa4b2b9f31b5c6d0b683db81157a9226e17b06e61911791ab547242a4a0556f2.css integrity="sha256-+ksrnzG1xtC2g9uBFXqSJuF7BuYZEXkatUckKkoFVvI=" media=screen crossorigin=anonymous><meta property="og:title" content="Vision Transformer 在目标检测上的探索，DETR 系列文章解读（三）Effient DETR, PnP DETR, Sparse DETR"><meta property="og:description" content="前言 这篇文章主要分享几篇 DETR 和 Deformable DETR 的后续工作，包括 Effient DETR [1], PnP DETR [2], Sparse DETR [3]。 分享这几篇工作的主要原因在于它们都是 DETR 的一个方向的改进，即减少 DETR 中的"><meta property="og:type" content="article"><meta property="og:url" content="https://kinredon.github.io/post/vision-transformer-for-object-detection_3/"><meta property="article:section" content="post"><meta property="article:modified_time" content="2022-01-16T15:09:43+08:00"><meta itemprop=name content="Vision Transformer 在目标检测上的探索，DETR 系列文章解读（三）Effient DETR, PnP DETR, Sparse DETR"><meta itemprop=description content="前言 这篇文章主要分享几篇 DETR 和 Deformable DETR 的后续工作，包括 Effient DETR [1], PnP DETR [2], Sparse DETR [3]。 分享这几篇工作的主要原因在于它们都是 DETR 的一个方向的改进，即减少 DETR 中的"><meta itemprop=dateModified content="2022-01-16T15:09:43+08:00"><meta itemprop=wordCount content="3649"><meta itemprop=keywords content="Object Detection,Vision Transformer,DETR,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Vision Transformer 在目标检测上的探索，DETR 系列文章解读（三）Effient DETR, PnP DETR, Sparse DETR"><meta name=twitter:description content="前言 这篇文章主要分享几篇 DETR 和 Deformable DETR 的后续工作，包括 Effient DETR [1], PnP DETR [2], Sparse DETR [3]。 分享这几篇工作的主要原因在于它们都是 DETR 的一个方向的改进，即减少 DETR 中的"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>kinredon</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://kinredon.github.io/>Home</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://kinredon.github.io/post/>Archives</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://kinredon.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://kinredon.github.io/categories/>Categories</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://kinredon.github.io/about/>About</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>kinredon</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://kinredon.github.io/>Home</a></li><li class=menu-item><a class=menu-item-link href=https://kinredon.github.io/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=https://kinredon.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://kinredon.github.io/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=https://kinredon.github.io/about/>About</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>Vision Transformer 在目标检测上的探索，DETR 系列文章解读（三）Effient DETR, PnP DETR, Sparse DETR</h1><div class=post-meta><time datetime=2022-01-16 class=post-time>2022-01-16</time><div class=post-category><a href=https://kinredon.github.io/categories/detr/>DETR</a></div><span class=more-meta>约 3649 字</span>
<span class=more-meta>预计阅读 8 分钟</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#前言>前言</a></li><li><a href=#effient-detr-1--org7bad8a0>Effient DETR [<a href=#org7bad8a0>1</a>]</a></li><li><a href=#pnp-detr-2--org9427615>PnP DETR [<a href=#org9427615>2</a>]</a></li><li><a href=#sparse-detr-3--org2943052>Sparse DETR [<a href=#org2943052>3</a>]</a></li><li><a href=#总结>总结</a></li><li><a href=#bibliography>Bibliography</a></li></ul></nav></div></div><div class=post-content><h2 id=前言>前言</h2><p>这篇文章主要分享几篇 DETR 和 Deformable DETR 的后续工作，包括 Effient DETR [<a href=#org7bad8a0>1</a>], PnP DETR [<a href=#org9427615>2</a>], Sparse DETR [<a href=#org2943052>3</a>]。 分享这几篇工作的主要原因在于它们都是 DETR 的一个方向的改进，即减少 DETR 中的计算量。DETR 利用 transformer 实现了 end-to-end 的目标检测，不需要任何额外的后处理，如 NMS, 直接从 raw images 得到目标框。DETR 首先利用 CNN 提取的 feature, 然后把二维的 feature 拍扁后得到序列长度为 \(L=W*H\) 的 feature, 然后把它们输入到 transformer 模块中， transformer 计算 L 个 feature 之间的 attention, 计算量十分庞大，复杂度相对 feature map 的长宽来说是平方级别的。Deformable DETR 通过压缩 key 值，计算每个 query 对应的 reference point 来采样新的 key, 采样的点通常特别少，如 \(K=4\), 把 DETR 中 self-attention 复杂度从平方级别下降到了线性级别。关于 DETR 和 Deformable DETR 大家可以阅读我之前的两篇文章。</p><p><a href=https://zhuanlan.zhihu.com/p/449783484>Vision Transformer 在目标检测上的探索，DETR 系列文章解读（一）</a></p><p><a href=https://zhuanlan.zhihu.com/p/450121658>Vision Transformer 在目标检测上的探索，DETR 系列文章解读（二）Deformable DETR</a></p><p>今天分享的这些 DETR variants 主要思想就是压缩 DETR 模型的计算量，同时提升模型的检测精度。Effient DETR 通过给 object query 和 reference point 更好的初始化从而减少 decoder 的层数；PnP DETR 和 Sparse DETR 通过从 CNN 出来的 feature map 中选择 salient feature, 仅计算被选择的 feature 之间的 self-attention, 从而减少计算量，差别主要在于选择的方式不一样；</p><h2 id=effient-detr-1--org7bad8a0>Effient DETR [<a href=#org7bad8a0>1</a>]</h2><p>这篇工作主要探寻一个问题，就是为什么 DETR 需要 6 层 decoder 去更新 object query? 能不能仅用 1 层 decoder 也能达到 6 层的效果？针对这个问题，文章中做了一系列的观察实验。</p><p>作者首先分析了 encoder 的层数， decoder 的层数，以及 decoder 的 auxiliary loss 的重要性，如下表所示：
<img src=/ox-hugo/pngpaste_clipboard_file_20220115161601.png alt></p><p>发现 encoder 层数其实没那么重要， 3 层的 encoder 和 1 层的 encoder 相比，掉的点还是比较少的 (41.5 v.s. 39.8)，但是 decoder 的层数和 auxiliary loss 对模型的影响就比较大了。作者指出造成这样的结果主要原因是因为，decoder 的 auxiliary loss 给 decoder 提供了很强的监督用于更新 object query. 在我看来 encoder 就是一个特征提取的作用，前面其实有一个 ResNet 网络，特征提取能力也不错了，而 object query 直接与预测结果相关联，但是只有 decoder 能更新它，主要的监督就是来自 auxiliary loss.</p><p>文章进一步探讨了如何初始化 object query. 由于 object query 是一堆抽象的 feature, 虽然学习到了一些固定的 pattern, 但是不好直接探索它的初始化和最终学习到的 object query 是什么。作者首先感谢了一波 deformable detr 提出了 reference point 的概念， reference point 是由 object query 通过 linear 层直接预测得到的。通过观察 reference point 在图像中的分布，进一步分析 object query.</p><link rel=stylesheet href=/css/hugo-easy-gallery.css><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/ox-hugo/pngpaste_clipboard_file_20220115161738.png></div><a href=/ox-hugo/pngpaste_clipboard_file_20220115161738.png itemprop=contentUrl></a></figure></div><p>上图是 deformable detr 中初始阶段和训练了几个阶段的 reference point 分布图，可以看到，初始的时候基本均匀的分布在整个图像中，但在最后会聚焦于一些 object 上。</p><p>所以作者就想到了去探索 reference point 的初始化对结果的影响，如下表：</p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/ox-hugo/pngpaste_clipboard_file_20220115161850.png></div><a href=/ox-hugo/pngpaste_clipboard_file_20220115161850.png itemprop=contentUrl></a></figure></div><p>主要的观察就是 6 层的 decoder 无论如何初始化，最终都能学得不错。但是 1 层的 decoder 被初始化影响较大，表中的 dense 就是后面作者提出的方法，后面会详细展开说。</p><p>基于以上的观察，作者提出了一个新的方法 Efficient DETR, 只有三层的 encoder 和 1 层的 decoder 在 COCO 上达到 44.2 的 mAP, 并且训练时间更快 (36 epochs).</p><p>Efficient DETR 主要包含两个部分： dense 和 sparse, 如下图所示：
<img src=/ox-hugo/pngpaste_clipboard_file_20220115161915.png alt></p><p>在 CNN 出来的 dense feature map 上预测 top score 的 proposals, 这些 proposal 的 2-d 或者 4-d 坐标用于初始化 reference point. 除此之外，再选择 top-k 的 feature 来初始化 object query.</p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/ox-hugo/pngpaste_clipboard_file_20220115164352.png></div><a href=/ox-hugo/pngpaste_clipboard_file_20220115164352.png itemprop=contentUrl></a></figure></div><p>实验效果如上表所示，仅用 3 个 encoder 和 1 个 decoder, 训练 36 个 epochs 便能在 COCO 上达到 \(44.2\) 的 mAP.</p><h2 id=pnp-detr-2--org9427615>PnP DETR [<a href=#org9427615>2</a>]</h2><p>这篇工作的作者认为 DETR 的 Transformer 网络在计算注意力时计算量过高的主要原因在于图像 feature map 的空间特征具有冗余性，即使用 feature map 中所有空间位置的特征来计算 self-attention 是不必要的，可以挑选一些 salient feature 用于计算。因此作者提出了一个两步的 poll-and-pool 采样模块，poll 模块用于在 feature map 中细粒度地采样前景 feature，然后对未采样的 feature 使用 pool 的机制将它们聚合在几个具有 contextual 信息的粗粒度 feature vector 上。经过这两个操作后得到新的 feature set, 基于这个新的 feature set 计算 self-attention.</p><p>作者构建了一个 PnP-DETR，PnP-DETR 仅在细粒度的前景 feature 和 粗粒度的背景 feature 之间计算注意力，大大的减少计算量。作者使用一个比例 \(\alpha\) 来确认前景 feature 的个数，固定的 M 值确定背景环境粗粒度 feature 的个数。PnP-DETR 可以实现训练一个模型后调整 \(\alpha\) 达到不同的推理速度。同时，PnP 采样模块比较通用，是一个端到端可学习不需要向 RPN 一样额外监督的模块，可以应用到别的 transformer 框架中，如 ViT。</p><p>为了得到这两个集合，作者提出 Poll and Pool (PnP) Sampling 策略，poll sampler 从 CNN 得到的 feature map 中找到前景 feature, 具体来说， pool sampler 使用一个 meta-scoring 网络预测 fature map 所有位置中哪些 feature 是最 informative 的，最终取 \(N=\alpha L\) 个 vector 作为细粒度的前景 feature set。这样就产生一个问题，这个 meta-scoring 网络要怎么学习呢？作者采用的方式是把预测的 score 作为 feature 的一个 modulating factor, 即：
<img src=/ox-hugo/pngpaste_clipboard_file_20220115162311.png alt></p><p>通过这样的方式，作者让 meat-scoring 网络的学习做到了端到端，不需要向 RPN 那样用一个额外的显式地监督来学习，都隐含在最终的任务中了。</p><p>对于没有被采样的 feature, 作者认为这一部分包含了很多物体周围的环境信息，但由于计算量的考虑，不应该都用来计算注意力机制。所以作者提出了一个 pool sampler 采样，即用一个可学习的权重来聚合这些背景 feature, 最终得到 M 个 feature. 作者将 poll 和 pool 得到的 feature 作为一个集合，用于后续的注意力计算，通过采样将原本 \(L=H*W\) 个 feature 转化到了 \(N + M\) 个，减少了很多计算量。</p><p>除了检测任务，作者还希望让模型能够做 Dense Prediction 任务，比如 segmentation, 但是由于 feature 经过了采样，不太适合这类任务。针对这个问题，作者提出 Reverse Projection 的操作，将采样得到的 feature 扩散回原来的 feature map. 对于 poll 的 feature, 直接安排到原来的采样位置就好。那对于经过权重矩阵聚合的 pool feature 怎么办呢？作者的方法也很简单粗暴，再用一个权重矩阵映射回去就行了。</p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/ox-hugo/pngpaste_clipboard_file_20220115164809.png></div><a href=/ox-hugo/pngpaste_clipboard_file_20220115164809.png itemprop=contentUrl></a></figure></div><p>实验结果在 DETR 中不算很高，在 res50 上能达到 43.1 的 mAP, 同时减少了 50% 的计算量。当减少 66% 时，能达到 42.7 的效果，值得注意的是单尺度的 deformable DETR 也能达到 42.1 的结果，但 Pnp DETR 计算量更少。除此之外，这个方法还有一个优点，作者提出的这个策略通过调整采样率能够在推理的时候动态地调整，从而改变推理速度并使得精度波动不大。</p><h2 id=sparse-detr-3--org2943052>Sparse DETR [<a href=#org2943052>3</a>]</h2><p>这篇工作提出 Sparse DETR, 主要的 motivation 是解决 DETR 方法中 encoder 计算量过大的问题。假设 feature map 大小为 H*W, 那么转化为序列长度为 L=H*W, DETR 计算两两之间的 attention, 复杂度是 N 的平方。Deformable DETR 使用 deformable attention 代替 DETR 中的全局 attention, 稀疏化 key 从而减少计算量，从平方复杂度降低到了线性复杂度，N*k （k&#171;N, k=4, in deformable DETR)，才使得模型可以使用多尺度的 feature, 进一步来提升模型的性能。但是由于多尺度的引入，encoder 的 tokens(query) 增加了很多，因此计算消耗仍然是 encoder attention 的瓶颈。</p><p>同时，作者观察到其实仅仅更新 encoder 中的一部分 tocken 并不会造成太大的性能下降，因此作者提出稀疏化 encoder tokens(query), 具体的 attention 复杂度比较如下图所示：
<img src=/ox-hugo/pngpaste_clipboard_file_20220115162501.png alt></p><p>其中黑点是自己和自己的 attention, 灰色的圈是保存下来的 attention 连接，白色的圈是去掉的 attention 连接，可以看出 Sparse DETR 相比 DETR 和 Deformable DETR 降低了很多的计算量。</p><p>为了实现 token 的稀疏化，作者提出两个方法来挑选 saliency 区域，分别是 Objectness Score 和 Decoder Cross-Attention Map. 除此之外，作者基于稀疏后的 token 提出两个额外的组件： Encoder Auxiliary Loss 和 Top-k Decoder Queries. 前者对 encoder 也使用辅助的 detection 监督损失，和 decoder 的 Auxiliary Loss 是一样的，可以这样的主要原因还是因为 encoder 的 token 被压缩了，不然对每个 token 都计算 auxiliary loss 的计算量太大了。后者就是把可学习的 decoder query 替换为从 encoder 中选择的 top-k token，和 PnP DETR 的方法差不多.</p><p>前面提到作者提出了两种方法来找到最 salient 的 encoder tokens: 分别是 Objectness Score 和 Decoder Cross-Attention Map.</p><p>Objectness Score 方法比较简单，就是在 feature map 后面添加一个检测头然后用 Hungarian loss 监督，这样选择分数比较高的用于后续 attention 计算，论文里面选择 \(top-\rho %\) 个 tokens. 这样的方法其实在很多检测方法中有使用，比如 RPN 就是在 feature map 上直接计算 objectness score. 尽管这样的方法能够找到一些 salient 的 encoder tokens, 但是这样的操作和 decoder 是独立的，没有充分考虑 decoder.</p><p>所以作者提出了 Decoder cross-Attention Map(DAM), 用 decoder 的 cross attention 来做为评价指标。直观上理解就是，对 object query 响应值高的 token, 天然地对最终的检测结果更为重要，所以应该保留用于计算得到最终的检测结果。如何得到这个 DAM 呢？ 对于 dense attention, 直接把 decoder 的每一层 cross attention 加在一起就可以；对于 deformable attention, 计算每个 object 的对应偏移后的 attention 就可以了。作者把这个 DAM 二值化，使用一个 scoring network 来预测这个 DAM, 用 BCE loss 来监督，如下：</p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/ox-hugo/pngpaste_clipboard_file_20220115162633.png></div><a href=/ox-hugo/pngpaste_clipboard_file_20220115162633.png itemprop=contentUrl></a></figure></div><p>其中 \({DAM}_i\) 是指第 \(i\) 个 encoder token 对应的 DAM.</p><p>这里有一个问题是在 feature map 后选择的 token 就确定了，未被选择的 token 就固定下来了，虽然 deformable attention 和它们会有交互，但是它们的值永远不会变，感觉可以考虑把这个选择的过程嵌入到每一个 encoder layer 后，感觉性能会有进一步的提升？</p><p>实验结果如下图，在更小计算量的同时性能超越 Deformable DETR. 作者将 ResNet 换成 Swin-T 时，效果更佳，达到了 \(49.3\) mAP.</p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/ox-hugo/pngpaste_clipboard_file_20220115164907.png></div><a href=/ox-hugo/pngpaste_clipboard_file_20220115164907.png itemprop=contentUrl></a></figure></div><h2 id=总结>总结</h2><p>这篇文章主要介绍了 Effient DETR, PnP DETR, Sparse DETR, 这几篇工作都在为减少 DETR 中计算 self-attention 的计算量努力，都取得了不错的效果。本文只能算抛砖引玉，对于想要深入研究的同学，还是需要仔细阅读原论文才行。除了本文介绍的工作，一篇工作提出 Adaptive Cluster Transformer (ACT) [<a href=#org5d30aa6>4</a>] 压缩 DETR 模型大小, 使得推理时计算量大大减少，ACT 主要通过计算 encoder 的 token 之间的相似性得到 prototype 从而压缩 token 数量，同时提出 DETR 模型的蒸馏方法提升压缩模型的性能，感兴趣的朋友也可以去看看原论文。</p><p>本篇文章这是 DETR 系列的第三篇文章，接下来将整理 DETR 的另一个优化方向的论文，通过加入更多的先验知识从而提升模型的收敛速度和精度，包括但不限于：</p><ul><li>Fast Convergence of DETR with Spatially Modulated Co-Attention(SMCA)</li><li>Anchor DETR: Query Design for Transformer-Based Detector</li><li>Conditional DETR for Fast Training Convergence</li></ul><p><strong><strong>写文不易，希望各位看官点个赞再走 vov</strong></strong></p><h2 id=bibliography>Bibliography</h2><p>[1] Z. Yao, J. Ai, B. Li, and C. Zhang, “Efficient DETR: Improving End-to-End Object Detector with Dense Prior,” <em>arXiv preprint arXiv:2104.01318</em>, 2021.</p><p>[2] T. Wang, L. Yuan, Y. Chen, J. Feng, and S. Yan, “PnP-DETR: towards efficient visual analysis with transformers,” in <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp. 4661–4670.</p><p>[3] B. Roh, J. Shin, W. Shin, and S. Kim, “Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity,” <em>arXiv preprint arXiv:2111.14330</em>, 2021.</p><p>[4] M. Zheng <em>et al.</em>, “End-to-end object detection with adaptive clustering transformer,” <em>BMVC</em>, 2021.</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>kinredon</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2022-01-16</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=https://kinredon.github.io/tags/object-detection/>Object Detection</a>
<a href=https://kinredon.github.io/tags/vision-transformer/>Vision Transformer</a>
<a href=https://kinredon.github.io/tags/detr/>DETR</a></div><nav class=post-nav><a class=prev href=/post/swimming/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">多飘一会儿</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/2021_conclusion/><span class="next-text nav-default">我的 2021 年总结</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697c-11.777231-11.500939-30.216186-10.304694-41.178865 2.712784z"/></svg></i></a></nav></footer></article><div class="post bg-white"><script src=https://utteranc.es/client.js repo=kinredon/comments-blog issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:kinredon@163.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408h399.992405s71.046998 3.997201 71.046998 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zm53.281707 130.131124C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523S0 1024 83.726336 1024H682.532949 753.579947h595.368192C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955c-39.771477 34.470494-74.671786 43.295855-98.955861 43.868928z"/></svg></a><a href=https://github.com/kinredon rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04C242.005334 929.664 211.968 830.08 211.968 830.08 188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://kinredon.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667v-199.04c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2022
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>kinredon</span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script>
<script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script>
<script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css integrity=sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js integrity=sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{})})</script><script type=text/javascript src=/js/load-photoswipe.js></script>
<script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script>
<script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script></body></html>