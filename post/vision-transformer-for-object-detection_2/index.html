<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Vision Transformer 在目标检测上的探索，DETR 系列文章解读（二）Deformable DETR - kinredon's blog</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="kinredon"><meta name=description content="前言 针对 DETR 收敛慢资源消耗多，小目标检测难的问题，Jifeng Dai 团队做出了 Deformable DETR，根据 Deformable convolution 的思想对 DETR 进行改进，即让注意力模块只注意到参考点"><meta name=keywords content="blog,computer vision,deep learning"><meta name=generator content="Hugo 0.101.0"><link rel=canonical href=https://kinredon.github.io/post/vision-transformer-for-object-detection_2/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.fa4b2b9f31b5c6d0b683db81157a9226e17b06e61911791ab547242a4a0556f2.css integrity="sha256-+ksrnzG1xtC2g9uBFXqSJuF7BuYZEXkatUckKkoFVvI=" media=screen crossorigin=anonymous><meta property="og:title" content="Vision Transformer 在目标检测上的探索，DETR 系列文章解读（二）Deformable DETR"><meta property="og:description" content="前言 针对 DETR 收敛慢资源消耗多，小目标检测难的问题，Jifeng Dai 团队做出了 Deformable DETR，根据 Deformable convolution 的思想对 DETR 进行改进，即让注意力模块只注意到参考点"><meta property="og:type" content="article"><meta property="og:url" content="https://kinredon.github.io/post/vision-transformer-for-object-detection_2/"><meta property="article:section" content="post"><meta property="article:published_time" content="2021-12-16T00:00:00+00:00"><meta property="article:modified_time" content="2021-12-26T21:57:40+08:00"><meta itemprop=name content="Vision Transformer 在目标检测上的探索，DETR 系列文章解读（二）Deformable DETR"><meta itemprop=description content="前言 针对 DETR 收敛慢资源消耗多，小目标检测难的问题，Jifeng Dai 团队做出了 Deformable DETR，根据 Deformable convolution 的思想对 DETR 进行改进，即让注意力模块只注意到参考点"><meta itemprop=datePublished content="2021-12-16T00:00:00+00:00"><meta itemprop=dateModified content="2021-12-26T21:57:40+08:00"><meta itemprop=wordCount content="1292"><meta itemprop=keywords content="Object Detection,Vision Transformer,DETR,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Vision Transformer 在目标检测上的探索，DETR 系列文章解读（二）Deformable DETR"><meta name=twitter:description content="前言 针对 DETR 收敛慢资源消耗多，小目标检测难的问题，Jifeng Dai 团队做出了 Deformable DETR，根据 Deformable convolution 的思想对 DETR 进行改进，即让注意力模块只注意到参考点"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>kinredon</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://kinredon.github.io/>Home</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://kinredon.github.io/post/>Archives</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://kinredon.github.io/tags/>Tags</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://kinredon.github.io/categories/>Categories</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://kinredon.github.io/about/>About</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>kinredon</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://kinredon.github.io/>Home</a></li><li class=menu-item><a class=menu-item-link href=https://kinredon.github.io/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=https://kinredon.github.io/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=https://kinredon.github.io/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=https://kinredon.github.io/about/>About</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>Vision Transformer 在目标检测上的探索，DETR 系列文章解读（二）Deformable DETR</h1><div class=post-meta><time datetime=2021-12-16 class=post-time>2021-12-16</time><div class=post-category><a href=https://kinredon.github.io/categories/detr/>DETR</a></div><span class=more-meta>约 1292 字</span>
<span class=more-meta>预计阅读 3 分钟</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#前言>前言</a></li><li><a href=#正文>正文</a><ul><li><a href=#deformable-attention-module>Deformable Attention Module</a></li><li><a href=#multi-scale-deformable-attention-module>Multi-scale Deformable Attention Module</a></li><li><a href=#deformable-transformer-encoder>Deformable transformer encoder</a></li><li><a href=#deformable-transformer-decoder>Deformable transformer decoder</a></li><li><a href=#deformable-detr-的两个变体>Deformable DETR 的两个变体</a></li></ul></li><li><a href=#总结>总结</a></li></ul></nav></div></div><div class=post-content><h2 id=前言>前言</h2><p>针对 DETR 收敛慢资源消耗多，小目标检测难的问题，Jifeng Dai 团队做出了 Deformable DETR，根据 Deformable convolution 的思想对 DETR 进行改进，即让注意力模块只注意到参考点周围的少量样本点。如下图所示，作者提出的 Deformable DETR 把多尺度的 feature map 送入 encoder 中，在 encoder 中，下一层的 feature 由上一层的参考点周围的少量点决定，最终将 encoder 的多次度特征再送到 decoder 中，最终输出预测的结果。</p><link rel=stylesheet href=/css/hugo-easy-gallery.css><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/ox-hugo/pngpaste_clipboard_file_20211216192053.png></div><a href=/ox-hugo/pngpaste_clipboard_file_20211216192053.png itemprop=contentUrl></a></figure></div><h2 id=正文>正文</h2><p>Deformable DETR 的核心在于 Deformable Attention Module，该模块用于直接替换 DETR 中的 multi-head self-attention 模块，由于 Deformable DETR 在计算 attention 的时候只考虑参考点周围的几个偏移点，大大的减少了计算量，因此也可以使得模型能够扩展到 multi-scale，进一步提升模型在不同尺度目标上的检测性能。</p><h3 id=deformable-attention-module>Deformable Attention Module</h3><p>Deformable DETR 的主要贡献便是提出了一个 Deformable Attention Module 的模块，该模块可以用于替换 encoder 中的 self-attention 模块，能极大地减少计算量。由于 query feature 仅与参考点周围的 K 个样本做交互，实现中 K=4，因此计算量大大减少。减少计算量的同时，由于限制了 attention 的范围，相当于加入了一些先验知识，使得模型收敛更快。</p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/ox-hugo/pngpaste_clipboard_file_20211216194118.png></div><a href=/ox-hugo/pngpaste_clipboard_file_20211216194118.png itemprop=contentUrl></a></figure></div><p>Deformable Attention Module 的具体设计入上图所示，首先一个 Query Feature \(z_q\) 经过两个 linear 层，分别生成 \(2*MK\) 和 \(MK\) 的 offset 和 attention weight。根据对应的参考点（其实就是 query 在 feature map 中的位置）预测 offset，计算新的坐标点，采集这些新坐标点特征并用 attention weight 进行加权融合信息，最终将多个 head 的信息 concat 到一起。形式化的定义如下：</p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/ox-hugo/pngpaste_clipboard_file_20211225163158.png></div><a href=/ox-hugo/pngpaste_clipboard_file_20211225163158.png itemprop=contentUrl></a></figure></div><p>其中 x 是一个 \(C*H*W\) 的 feature map， \(\boldsymbol{W}_{m}^{\prime}\) 是一个 1*1 的卷积层，用于降低 channel 的维度， \(\boldsymbol{p}_{q}\) 是参考点， \(\Delta \boldsymbol{p}_{m q k}\) 是用 query feature 预测得到的 offset，从feature map 中得到参考点 \(\boldsymbol{p}_{q}\) 周围的特征，通过 attention weight 加权求和得到新的 feature。</p><h3 id=multi-scale-deformable-attention-module>Multi-scale Deformable Attention Module</h3><p>为了解决多尺度目标检测的问题，作者提出了 Multi-scale Deformable Attention Module，主要的差别是将不同层的 feature 融合在一起，多了一个 scale 的维度，如下：</p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/ox-hugo/pngpaste_clipboard_file_20211225163249.png></div><a href=/ox-hugo/pngpaste_clipboard_file_20211225163249.png itemprop=contentUrl></a></figure></div><p>其中 \(\phi_{l}\) 是一个映射函数，将参考点的位置映射回对应的 feature map 层级。</p><h3 id=deformable-transformer-encoder>Deformable transformer encoder</h3><p>在实现上，作者使用了四层的 feature map 作为多尺度特征，前三层是 ResNet 的 \(C_3\) 和 $C_5$，最后一层通过在最后的 \(C_5\) 层后加一个 \(3*3，stride=2\) 的卷积层得到，记为 \(C_6\) 。所有的多尺度 feature maps 都转化为同一个 channel 大小 \(C=256\) ，入下图所示：</p><div class=box><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/ox-hugo/pngpaste_clipboard_file_20211216200200.png></div><a href=/ox-hugo/pngpaste_clipboard_file_20211216200200.png itemprop=contentUrl></a></figure></div><p>为了在学习过程中，模型能够知道对应的特征来着哪一个层级，作者在 positional encoding 里面还加入了随机初始化科学系的 scale-level 的 positional encoding。</p><h3 id=deformable-transformer-decoder>Deformable transformer decoder</h3><p>在 decoder 中，每一个 decoder layer 有 self-attention 和 cross-attention 两个部分，self-attention可以直接从计算 object query。而 object query 在 cross-attention 对应的参考点则使用一个可学习的 linear layer 来学习。由于最终抽取的特征是在一个参考点周围的图像特征，所以作者将检测头也设计为预测针对参考点的 offset 而不是绝对坐标。</p><h3 id=deformable-detr-的两个变体>Deformable DETR 的两个变体</h3><p>作者还设计了两个 Deformable DETR 的变体，分别是进行迭代式的 bounding boxes 修正，以及 two-stage Deformable DETR。</p><ol><li>迭代式的 bounding boxes 修正</li></ol><p>decoder 的每一层修正上一层的预测，即上一层的预测的结果输入到下一层。</p><ol><li>Two-Stage Deformable DETR</li></ol><p>从 two-stage 目标检测器得到启发，作者首先利用一个 FFN 在 encoder 的 feature 上得到初始的 region proposal，挑选其中分数较高的 proposal 用于后续的优化。具体来说，作者将分数较高的 proposal 当做迭代式的 bounding boxes 修正的初始点。</p><h2 id=总结>总结</h2><p>Deformable DETR 通过将 Deformable Conv 的思想引入 DETR，不仅减少了计算量，而且提高了模型的收敛速度。在实验部分，Deformable DETR 仅需要 50 epochs 便能达到 DETR 需要 500 epochs 达到的效果，相当于 10x 的收敛速度，而且精度更高。</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>kinredon</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2021-12-26</span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=https://kinredon.github.io/tags/object-detection/>Object Detection</a>
<a href=https://kinredon.github.io/tags/vision-transformer/>Vision Transformer</a>
<a href=https://kinredon.github.io/tags/detr/>DETR</a></div><nav class=post-nav><a class=prev href=/post/2021_conclusion/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">我的 2021 年总结</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/vision-transformer-for-object-detection/><span class="next-text nav-default">Vision Transformer 在目标检测上的探索，DETR 系列文章解读（一）</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697c-11.777231-11.500939-30.216186-10.304694-41.178865 2.712784z"/></svg></i></a></nav></footer></article><div class="post bg-white"><script src=https://utteranc.es/client.js repo=kinredon/comments-blog issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div></div></main><footer id=footer class=footer><div class=icon-links><a href=mailto:kinredon@163.com rel="me noopener" class=iconfont title=email><svg class="icon" viewBox="0 0 1451 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408h399.992405s71.046998 3.997201 71.046998 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zm53.281707 130.131124C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523S0 1024 83.726336 1024H682.532949 753.579947h595.368192C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955c-39.771477 34.470494-74.671786 43.295855-98.955861 43.868928z"/></svg></a><a href=https://github.com/kinredon rel="me noopener" class=iconfont title=github target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="36" height="36"><path d="M512 12.672c-282.88.0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667.0-12.16-.426667-44.373333-.64-87.04C242.005334 929.664 211.968 830.08 211.968 830.08 188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333.0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333.0.0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667.0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72.0 68.522667-.64 123.562667-.64 140.202666.0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"/></svg></a><a href=https://kinredon.github.io/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667v-199.04c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2021 -
2022
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>kinredon</span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script>
<script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script>
<script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css integrity=sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js integrity=sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{})})</script><script type=text/javascript src=/js/load-photoswipe.js></script>
<script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script>
<script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script></body></html>